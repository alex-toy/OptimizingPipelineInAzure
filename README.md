# Optimizing an ML Pipeline in Azure

## Overview
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
This dataset contains data about the direct marketing campaigns of a Portuguese banking institution and this dataset is from [UCL ML Repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing). The goal of this project is to predict which of the bank's clients will subscribe to term deposit. As such, this is a binary classification task which requires predicting 'yes' or 'no' based on the client data.

For the classification, two approaches have been used:

- Scikit-learn **Logistic Regression** using a custom coded model where the hyperparameters are fine tuned using AzureML's **Hyperdrive**, which is able to achieve an accuracy of 0.908%
- Azure **AutoML** (the 'no code/low code' solution), which uses a variety of machine learning models to arrive at the best performing acccuracy of xx%, tad better than the Hyperdrive method, with the **VotingEnsemble** model.


## Scikit-learn Pipeline
Those first three steps take place in the *train.py* script. Then the hyperparameters of the Logistic Regression model are fine tuned thanks to Hyperdrive.
The core components of this pipeline include :
1. dataset is downloaded in tabular format using the TabularDatasetFacory class,
2. data goes through the preprocessing steps, which includes cleaning, preparing the data, splitting the data,
3. Applying Logistic Regression model


4. Passing this model through Hyperdrive
5. Picking the best model

From execution standpoint, jupyter notebook will consume the train.py file to generate the best model by passing the Azure ML Hyperdrive to fine tune the parameters. the hyperparameters that I have decided to fine tune are :
- 'C' - inverse of regularisation strength, with positive floats uniformy chosen in a range from 0.001 to 0.0001. Smaller values specify stronger regularisation, as per the Logistic Regression documentation in SKLearn.
- 'max-iter' - the maximum number of iterations taken by the solver to converge, which I chose in a range of 50-150.


**Benefits of the parameter sampler :**
As part of this project, I chose to use **RandomParameterSampling** to sample over a hyperparameter search space. In this algorithm, parameter values are chosen from a set of discrete values or a distribution over a continuous range. Examples of functions you can use include: choice, uniform, loguniform, normal, and lognormal. This method uses less computation resources and takes less time, compared to other sampling methods such as grid sampling. This is due to the fact that random sampling randomly selects hyperparameter values from the defined seach space, while grid sampling performs a simple but exhaustive grid search over all possible values. This idea of random sampling can be captured easily with this 3D representation from my run :

<img src="/pictures/3Dsearch.PNG"  width="500">

**Benefits of the early stopping policy :**
The Bandit policy saves time and computation resources in running models that do not perform well, since it stops those models early, as soon as they are showing bad results. It is based on a slack factor. If the accuracy of the current run isn't within the slack amount of the best run, the current run is terminated, allowing for some saving on comptational time/compute usage.

![Alt text](/pictures/primarymetric.PNG "a title")


## AutoML
**Description of the model and hyperparameters generated by AutoML :**
AutoML (Automated Machine Learning) essentially automates all aspects of the machine learning process i.e, feature engineering, selection of hyperparameters, model training etc. It uses a variety of models including classification, regression and others for training. The hyperparameters include:
- A timeout of 30 minutes to save on usage cost
- The primary task being *classification* since that is the goal of the project
- The primary metric set to 'accuracy' to score the model (and comparison with Hyperdrive)
- The same 'cpu_cluster' compute cluster being used to perform the training like in Hyperdrive
- The target/predicion labels being 'y' like standard prediction tasks
- Number of cross validations set to 5
- ONNX compatibility set to true, allowing for interchange of models

The best model selected by the AutoML is VotingEnsemble model, which is based on 7 different ensemble models each with specific weightage as shown below:

<img src="/pictures/models.png"  width="200">

<img src="/pictures/ROC.png"  width="800">

<img src="/pictures/CM.png"  width="800">

## Pipeline comparison
**Comparison of the two models and their performance / differences in accuracy / In architecture**
The AutoML performs better than the Hyperdrive, even if the improvement in accuracy is only about 1%-2%. This is due to the fact that **AutoML** has a number of models at its disposal as compared to **Hyperdrive**.


## General steps to reproduce the whole procedure :

1. Create a workspace.
2. Create an compute instance. From the instance Jupyter, upload the notebooks.
3. Create a compute cluster.
4. Upload Data.
5. Run the notebooks.

## Future work / areas of improvement for future experiments :

- HyperDrive:
    1. Different classification models with HyperDrive.
    2. Wider range of hyper parameters in the HyperDrive

- AutoML
    1. Different sampling methods such as Grid search.
    2. Different termination policies and compare how it performs.